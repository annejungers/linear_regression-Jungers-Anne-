{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGRCdv2_63Fm"
   },
   "source": [
    "# Linear regression\n",
    "\n",
    "Now that we've created our first learning machine model, let's see how it works under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IHjwpyJmYz0"
   },
   "source": [
    "## How does it work?\n",
    "Here comes a part that some of you fear: Mathematics!    \n",
    "\n",
    "But don't worry, you'll see that it's not that complicated.\n",
    "\n",
    "### How to calculate the y-axis from the x-axis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPLnFMBxs4C9"
   },
   "source": [
    "A linear model is in fact based on a simple [affine function.](https://fr.wikipedia.org/wiki/Fonction_affine) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XTFIZfapioo"
   },
   "source": [
    "$$f(x) = ax + b$$\n",
    "or ...\n",
    "$$y = f(x) = ax + b$$\n",
    "or..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "y = a*x + b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Create a function ``f`` which receives as a parameter the variables ``x``,``a`` and ``b`` and returns ``y``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x, a, b):\n",
    "    y = a*x + b\n",
    "    return y\n",
    "\n",
    "f(1, 4, 5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will allow us to create a straight line that passes through all the points as well as possible. For the moment, we do not know the value of parameters a and b, so it is impossible to draw a good straight line on the scatter plot, unless we choose parameters at random. And that is what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl6jA7JTdoO8"
   },
   "source": [
    "\n",
    "The linear model with random parameters would look something like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Va6jerdvjC"
   },
   "source": [
    "![image.png](./assets/random_bias.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x4DbSIbeQNA"
   },
   "source": [
    "But we want to achieve this result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7hNlrnOezsy"
   },
   "source": [
    "![](./assets/trained_bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKo1rgh_fA7O"
   },
   "source": [
    "And it will be the role of the machine to learn how to find these values (a and b) by minimizing the cost function that we will see in detail in the next chapter.\n",
    "\n",
    "Before we do that, we need to look at the small problem we have with this method. If we execute the function as is, we would have to make a loop for each element $x^{i}$ of our dataset. This can be very expensive in terms of machine resources. If your dataset is very large, it will take a lot of time to train your model. \n",
    "\n",
    "To solve this problem, it is usual to use something you are beginning to know, matrices! \n",
    "\n",
    "The matrices allow us to perform the function only once on our entire dataset. \n",
    "\n",
    "The matrix writing of $f(x)=ax+b$ is written like this:\n",
    "$$ F = X . \\theta$$\n",
    "\n",
    "As these are matrices that contain all the data, by convention, we put them in uppercase.\n",
    "\n",
    "The variable $F$ will contain a matrix with the set of predictions of $x^{(i)}$. \n",
    "\n",
    "$$ \n",
    "F \\\\\n",
    "\\begin{bmatrix}\n",
    "f(x^{(1)})\\\\\n",
    "f(x^{(2)})\\\\\n",
    "f(x^{(3)})\\\\\n",
    "... \\\\\n",
    "f(x^{(m)})\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $\\theta$ (pronounced theta) will contain a vector with the values $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta \\\\\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $X$ will contain a matrix with two dimensions, one dimension with the value of $x^{(i)}$ and another dimension with 1's everywhere.  Why? Because we have to multiply our X and theta matrices.\n",
    "$$ \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "x^{(1)} && 1\\\\\n",
    "x^{(2)} && 1\\\\\n",
    "x^{(3)} && 1\\\\\n",
    "... \\\\\n",
    "x^{(m)} && 1\\\\\n",
    "\\end{bmatrix}\n",
    ".\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "![](./assets/dot_mat.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which amounts to writing this: \n",
    "$$ y^{(1)} = x^{(1)}* a + 1 * b$$\n",
    "\n",
    "And if we simplify:\n",
    "$$ y^{(1)} = ax^{(1)} + b$$\n",
    "\n",
    "\n",
    "So we are back to our original function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :**\n",
    "1. Create a variable ``X`` which contains a matrix (30,2) with a column filled with values of our dataframe and then another one with 1's. \n",
    "2. Create the ``theta`` variable which contains a vector with 2 random values.\n",
    "3. Create a variable ``F`` which contains a multiplication of the matrix X with the theta vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Create a ``model`` function that receives as parameter ``X`` and ``theta``.  The function must return ``F``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.27179757]\n",
      " [4.35524969]\n",
      " [4.4387018 ]\n",
      " [4.64733208]\n",
      " [4.73078419]\n",
      " [5.02286658]\n",
      " [5.06459263]\n",
      " [5.14804474]\n",
      " [5.14804474]\n",
      " [5.35667502]\n",
      " [5.44012713]\n",
      " [5.48185319]\n",
      " [5.48185319]\n",
      " [5.52357924]\n",
      " [5.69048347]\n",
      " [5.85738769]\n",
      " [5.9408398 ]\n",
      " [6.02429191]\n",
      " [6.27464824]\n",
      " [6.3163743 ]\n",
      " [6.65018274]\n",
      " [6.77536091]\n",
      " [7.10916936]\n",
      " [7.23434752]\n",
      " [7.4429778 ]\n",
      " [7.56815597]\n",
      " [7.77678625]\n",
      " [7.8185123 ]\n",
      " [8.11059469]\n",
      " [8.1940468 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/salary_data.csv\")\n",
    "\n",
    "X = np.empty([30, 2], dtype=float)\n",
    "X[:,0] = df.iloc[:, 0].values\n",
    "X[:,1] = 1     #filling another column with 1's\n",
    "\n",
    "\n",
    "theta = np.random.normal(size=2).reshape(2,1)\n",
    "\n",
    "F = np.dot(X, theta)\n",
    "print(F)\n",
    "\n",
    "#X = X.reshape(30,2)\n",
    "#X = np.arange(30, 2)\n",
    "#theta = np.random.random((a,b)).reshape\n",
    "#dot function numpy\n",
    "#F = X * theta # there is a function in numpy to do this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** Create a ``y_pred`` variable and use your `model` function with ``X, theta``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.dot(X, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to apply our model to our entire dataset. Now we have to know how to find the right values for a and b. For that we will have to calculate the average of all our errors with a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKaQ28Hafn_h"
   },
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRBuUaU2ftck"
   },
   "source": [
    "The cost function allows us to evaluate the performance of our model by measuring the errors between the prediction and the actual value. The question we ask ourselves is: How to measure these errors?\n",
    "\n",
    "Imagine that you have 4 years of experience and that you spend 90000€ per year. Your Machine Learning model predicts that this salary is worth €110000. You can conclude that your model therefore makes an error of 90000 - 110000 = -20000 €.\n",
    "\n",
    "Thus, you could say that to measure your errors, you have to calculate the difference $f(x)-y$. However, if your prediction f(x) is less than y, then this error is negative (as in the example above), and it is not very practical to minimize this function.\n",
    "\n",
    "So, to measure the errors between the $f(x)$ predictions and the y-values of the Dataset, we calculate the square of the difference: $(f(x)-y)^2$. This, by the way, is what is called the Euclidean norm, which represents the direct distance between $f(x)$ and y in Euclidean geometry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJFTEvD8wb7b"
   },
   "source": [
    "\n",
    "\n",
    "![image.png](./assets/eucli.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xa-c_X4xt2vO"
   },
   "source": [
    "But this is not enough. Indeed, we have the error of a single example. But we must have the average of all the errors of all the points. \n",
    "\n",
    "We could write it like this: \n",
    "\n",
    "\n",
    "\n",
    "$$MSE(a,b) = {\\dfrac{(f(x^{(1)})- y^{(1)})^2 + (f(x^{(2)})- y^{(2)})^2  + ... +(f(x^{(m)})- y^{(m)})^2}{m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GUWy24S5b27"
   },
   "source": [
    "Why $MSE$? Because this function is called **Mean Squared Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImWIompHxm2Q"
   },
   "source": [
    "By convention this function is written in the following way, adding a coefficient $\\frac{1}{2}$ to simplify a derivative calculation that will come later.\n",
    "\n",
    "$$ MSE(a, b) = {\\dfrac{1}{2m}} \\sum _ {i=1}^m (f(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$ MSE(a, b) = {\\dfrac{1}{2m}} \\sum _ {i=1}^m (ax^{(i)} +b - y^{(i)})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as we work with matrices, we also have to transcribe our formula which becomes : \n",
    "\n",
    "$$MSE(\\theta) = \\frac {1}{2m}  \\sum _ {i=1}^m (X . \\theta - y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or \n",
    "\n",
    "```py\n",
    "MSE = 1/(2*m) * sum((X * theta - y)**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47zckFxMQiVs"
   },
   "source": [
    "**Exercise :** Create a ``MSE`` function that receives as parameter ``X, y and theta`` using the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.25098867e+09]\n"
     ]
    }
   ],
   "source": [
    "m = len(df)\n",
    "\n",
    "y = np.empty([30,1])\n",
    "y[:,0] = df.iloc[:, -1].values.T\n",
    "\n",
    "MSE = 1/(2*m) * sum((y_pred - y)**2)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_2cfr2DOB9X"
   },
   "source": [
    "### Minimize the cost function.\n",
    "\n",
    "\n",
    "\n",
    "If the cost function is omitted with respect to the parameter, it looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASRXsJyO__ic"
   },
   "source": [
    "![image.png](./assets/convexe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55-cHhR_DBNI"
   },
   "source": [
    "The aim is therefore to reach the lowest point of the curve, i.e. the lowest possible sum of errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqyzIhPKDMc_"
   },
   "source": [
    "![image.png](./assets/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, there are several function minimization algorithms, such as the least squares method or **gradient descent**. We will focus here on gradient descent because it is one of the most widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gmDtf6l-f7T"
   },
   "source": [
    "Gradient descent is an iterative algorithm which therefore proceeds by progressive improvements. For a linear problem, this algorithm needs to have two hyper-parameters :\n",
    "\n",
    "**1. The number of iterations :** As its name indicates, this is the parameter that will determine the number of iterations.\n",
    "\n",
    "**2. The learning rate :** This is the length of the step between each iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMJ0V9J6HQZa"
   },
   "source": [
    "![learningrate](./assets/gradient_descent_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to clearly define the learning rate. If you set a high value, the algorithm will be faster, but you risk never reaching the lowest point of the curve, the steps being too big. Our model will never be able to work since it cannot find the minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TASxiF6zHnq0"
   },
   "source": [
    "![](./assets/gradient_descent_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, if you set a small value, then the algorithm will find the lowest point of the curve, but it will be slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9on9fx_9HhYb"
   },
   "source": [
    "![learning rate](./assets/gradient_descent_3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, we will have to calculate the regression slope. \n",
    "![](./assets/derivative.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in mathematics we calculate a slope with a [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative#:~:text=In%20mathematics%2C%20a%20partial%20derivative,vector%20calculus%20and%20differential%20geometry.). The symbol used to denote partial derivatives is $\\partial$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac {\\partial MSE(\\theta) }{\\partial \\theta}  = \\frac {1}{m} X^T.(X.\\theta - y)$$\n",
    "\n",
    "The $X^T$ is to transpose the matrix, just like in numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could translate this into code like this:\n",
    "\n",
    "```py\n",
    "1/m * X.T.dot(model(X, theta) - y)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, y, theta):\n",
    "    m = len(y)\n",
    "    return 1/m * X.T.dot(np.dot(X, theta) - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have to write the gradient descent. \n",
    "\n",
    "$$\\theta = \\theta - a .  \\frac {\\partial MSE(\\theta) }{\\partial \\theta}$$\n",
    "\n",
    "The variable $a$ is the learning rate. So at each iteration, we redefine theta. We do : ``theta`` - ``learning_rate`` multiplied by the partial derivative of mean squared error. You could translate this into code like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "theta = theta - learning_rate * grad(X, y, theta)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :**\n",
    "1. Create a `gradient_descent` function that receives as parameter ``X``, ``y``, ``theta``, ``learning_rate``, ``n_iterations``\n",
    "2. In the function, create a variable `cost_history` with a matrix filled with 0 and which has a length of `n_iterations`. We will use it to display the histogram of the model learning process.\n",
    "3. Create a loop that iterates up to ``n_iterations``\n",
    "4. In the loop, update ``theta`` with the formula of the gradient descent (The example above) \n",
    "5. In the loop, update ``cost_history[i]`` with the values of ``MSE(X,y,theta)``\n",
    "6. return `theta` and `cost_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta_final, cost_history = gradiente_descedent(X, y, theta, learning_rate, n_iterations)\n",
    "#print(theta_final)\n",
    "\n",
    "def mse(X, y, theta):\n",
    "    return 1/(2*m)*sum((np.dot(X, theta) - y)**2)\n",
    "\n",
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    cost_history = np.zeros(n_iterations)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        theta = theta - learning_rate * grad(X, y, theta)\n",
    "        cost_history[i] = mse(X, y, theta)\n",
    "        \n",
    "    return (theta, cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model\n",
    "\n",
    "Now that we know which algorithm is used to minimize the cost function, we train our model.   \n",
    "We define a number of iterations, and a learning step $\\alpha$, and here we go!\n",
    "\n",
    "Once the model is trained, we observe the results compared to our dataset.\n",
    "\n",
    "**Exercise :** Create variables `n_iterations` and `learning_rate`. \n",
    "The learning rate and the n_iterations are defined by looking a little. You have to try several things, there is no magic number. However, starting with 1000 iterations and a learning_rate of 0.01 is a good basis to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create variables ``theta_final``, ``cost history`` and instance ``gradient_descent()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 9876.04721566],\n",
       "        [22920.92718644]]),\n",
       " array([1.34440596e+09, 5.82839902e+08, 2.78547195e+08, 1.56871366e+08,\n",
       "        1.08125881e+08, 8.85064311e+07, 8.05193701e+07, 7.71784498e+07,\n",
       "        7.56936467e+07, 7.49507371e+07, 7.45047428e+07, 7.41779554e+07,\n",
       "        7.38994027e+07, 7.36407385e+07, 7.33906400e+07, 7.31445826e+07,\n",
       "        7.29007569e+07, 7.26584375e+07, 7.24173322e+07, 7.21773218e+07,\n",
       "        7.19383559e+07, 7.17004121e+07, 7.14634786e+07, 7.12275483e+07,\n",
       "        7.09926159e+07, 7.07586765e+07, 7.05257259e+07, 7.02937599e+07,\n",
       "        7.00627741e+07, 6.98327645e+07, 6.96037269e+07, 6.93756572e+07,\n",
       "        6.91485513e+07, 6.89224052e+07, 6.86972148e+07, 6.84729760e+07,\n",
       "        6.82496849e+07, 6.80273374e+07, 6.78059295e+07, 6.75854573e+07,\n",
       "        6.73659168e+07, 6.71473041e+07, 6.69296153e+07, 6.67128464e+07,\n",
       "        6.64969935e+07, 6.62820529e+07, 6.60680206e+07, 6.58548928e+07,\n",
       "        6.56426657e+07, 6.54313354e+07, 6.52208983e+07, 6.50113504e+07,\n",
       "        6.48026881e+07, 6.45949076e+07, 6.43880051e+07, 6.41819771e+07,\n",
       "        6.39768197e+07, 6.37725293e+07, 6.35691022e+07, 6.33665349e+07,\n",
       "        6.31648235e+07, 6.29639646e+07, 6.27639546e+07, 6.25647898e+07,\n",
       "        6.23664666e+07, 6.21689816e+07, 6.19723311e+07, 6.17765117e+07,\n",
       "        6.15815198e+07, 6.13873519e+07, 6.11940046e+07, 6.10014744e+07,\n",
       "        6.08097578e+07, 6.06188514e+07, 6.04287518e+07, 6.02394555e+07,\n",
       "        6.00509592e+07, 5.98632595e+07, 5.96763530e+07, 5.94902364e+07,\n",
       "        5.93049063e+07, 5.91203594e+07, 5.89365924e+07, 5.87536020e+07,\n",
       "        5.85713849e+07, 5.83899378e+07, 5.82092576e+07, 5.80293409e+07,\n",
       "        5.78501845e+07, 5.76717853e+07, 5.74941400e+07, 5.73172454e+07,\n",
       "        5.71410983e+07, 5.69656957e+07, 5.67910343e+07, 5.66171110e+07,\n",
       "        5.64439227e+07, 5.62714663e+07, 5.60997387e+07, 5.59287369e+07,\n",
       "        5.57584576e+07, 5.55888980e+07, 5.54200550e+07, 5.52519255e+07,\n",
       "        5.50845064e+07, 5.49177949e+07, 5.47517880e+07, 5.45864825e+07,\n",
       "        5.44218757e+07, 5.42579645e+07, 5.40947459e+07, 5.39322172e+07,\n",
       "        5.37703752e+07, 5.36092172e+07, 5.34487403e+07, 5.32889416e+07,\n",
       "        5.31298181e+07, 5.29713671e+07, 5.28135857e+07, 5.26564712e+07,\n",
       "        5.25000205e+07, 5.23442311e+07, 5.21891000e+07, 5.20346244e+07,\n",
       "        5.18808017e+07, 5.17276291e+07, 5.15751037e+07, 5.14232230e+07,\n",
       "        5.12719840e+07, 5.11213843e+07, 5.09714209e+07, 5.08220913e+07,\n",
       "        5.06733928e+07, 5.05253226e+07, 5.03778782e+07, 5.02310569e+07,\n",
       "        5.00848561e+07, 4.99392731e+07, 4.97943053e+07, 4.96499502e+07,\n",
       "        4.95062051e+07, 4.93630675e+07, 4.92205348e+07, 4.90786044e+07,\n",
       "        4.89372739e+07, 4.87965406e+07, 4.86564020e+07, 4.85168556e+07,\n",
       "        4.83778990e+07, 4.82395296e+07, 4.81017450e+07, 4.79645426e+07,\n",
       "        4.78279201e+07, 4.76918749e+07, 4.75564046e+07, 4.74215069e+07,\n",
       "        4.72871792e+07, 4.71534192e+07, 4.70202244e+07, 4.68875926e+07,\n",
       "        4.67555212e+07, 4.66240080e+07, 4.64930505e+07, 4.63626465e+07,\n",
       "        4.62327935e+07, 4.61034894e+07, 4.59747316e+07, 4.58465180e+07,\n",
       "        4.57188462e+07, 4.55917140e+07, 4.54651190e+07, 4.53390590e+07,\n",
       "        4.52135317e+07, 4.50885349e+07, 4.49640664e+07, 4.48401239e+07,\n",
       "        4.47167051e+07, 4.45938079e+07, 4.44714301e+07, 4.43495694e+07,\n",
       "        4.42282237e+07, 4.41073908e+07, 4.39870686e+07, 4.38672548e+07,\n",
       "        4.37479474e+07, 4.36291442e+07, 4.35108430e+07, 4.33930418e+07,\n",
       "        4.32757384e+07, 4.31589307e+07, 4.30426167e+07, 4.29267942e+07,\n",
       "        4.28114611e+07, 4.26966155e+07, 4.25822552e+07, 4.24683782e+07,\n",
       "        4.23549824e+07, 4.22420658e+07, 4.21296264e+07, 4.20176622e+07,\n",
       "        4.19061712e+07, 4.17951513e+07, 4.16846006e+07, 4.15745170e+07,\n",
       "        4.14648987e+07, 4.13557437e+07, 4.12470499e+07, 4.11388154e+07,\n",
       "        4.10310384e+07, 4.09237168e+07, 4.08168488e+07, 4.07104324e+07,\n",
       "        4.06044657e+07, 4.04989468e+07, 4.03938738e+07, 4.02892449e+07,\n",
       "        4.01850581e+07, 4.00813117e+07, 3.99780036e+07, 3.98751322e+07,\n",
       "        3.97726954e+07, 3.96706916e+07, 3.95691189e+07, 3.94679753e+07,\n",
       "        3.93672593e+07, 3.92669688e+07, 3.91671022e+07, 3.90676576e+07,\n",
       "        3.89686333e+07, 3.88700274e+07, 3.87718382e+07, 3.86740640e+07,\n",
       "        3.85767030e+07, 3.84797535e+07, 3.83832136e+07, 3.82870817e+07,\n",
       "        3.81913561e+07, 3.80960350e+07, 3.80011167e+07, 3.79065996e+07,\n",
       "        3.78124819e+07, 3.77187619e+07, 3.76254380e+07, 3.75325085e+07,\n",
       "        3.74399717e+07, 3.73478260e+07, 3.72560696e+07, 3.71647010e+07,\n",
       "        3.70737186e+07, 3.69831206e+07, 3.68929056e+07, 3.68030717e+07,\n",
       "        3.67136175e+07, 3.66245413e+07, 3.65358416e+07, 3.64475167e+07,\n",
       "        3.63595651e+07, 3.62719851e+07, 3.61847753e+07, 3.60979340e+07,\n",
       "        3.60114597e+07, 3.59253509e+07, 3.58396059e+07, 3.57542233e+07,\n",
       "        3.56692015e+07, 3.55845391e+07, 3.55002344e+07, 3.54162860e+07,\n",
       "        3.53326923e+07, 3.52494519e+07, 3.51665633e+07, 3.50840250e+07,\n",
       "        3.50018355e+07, 3.49199933e+07, 3.48384970e+07, 3.47573451e+07,\n",
       "        3.46765361e+07, 3.45960686e+07, 3.45159412e+07, 3.44361524e+07,\n",
       "        3.43567008e+07, 3.42775850e+07, 3.41988034e+07, 3.41203549e+07,\n",
       "        3.40422378e+07, 3.39644509e+07, 3.38869927e+07, 3.38098618e+07,\n",
       "        3.37330569e+07, 3.36565766e+07, 3.35804195e+07, 3.35045842e+07,\n",
       "        3.34290694e+07, 3.33538737e+07, 3.32789958e+07, 3.32044343e+07,\n",
       "        3.31301880e+07, 3.30562554e+07, 3.29826352e+07, 3.29093261e+07,\n",
       "        3.28363269e+07, 3.27636361e+07, 3.26912526e+07, 3.26191749e+07,\n",
       "        3.25474019e+07, 3.24759321e+07, 3.24047644e+07, 3.23338974e+07,\n",
       "        3.22633299e+07, 3.21930607e+07, 3.21230883e+07, 3.20534117e+07,\n",
       "        3.19840296e+07, 3.19149406e+07, 3.18461437e+07, 3.17776374e+07,\n",
       "        3.17094207e+07, 3.16414923e+07, 3.15738509e+07, 3.15064953e+07,\n",
       "        3.14394245e+07, 3.13726370e+07, 3.13061318e+07, 3.12399077e+07,\n",
       "        3.11739634e+07, 3.11082978e+07, 3.10429097e+07, 3.09777979e+07,\n",
       "        3.09129613e+07, 3.08483987e+07, 3.07841089e+07, 3.07200908e+07,\n",
       "        3.06563433e+07, 3.05928651e+07, 3.05296552e+07, 3.04667125e+07,\n",
       "        3.04040357e+07, 3.03416238e+07, 3.02794757e+07, 3.02175902e+07,\n",
       "        3.01559662e+07, 3.00946026e+07, 3.00334984e+07, 2.99726524e+07,\n",
       "        2.99120635e+07, 2.98517307e+07, 2.97916528e+07, 2.97318289e+07,\n",
       "        2.96722577e+07, 2.96129383e+07, 2.95538696e+07, 2.94950505e+07,\n",
       "        2.94364800e+07, 2.93781570e+07, 2.93200804e+07, 2.92622493e+07,\n",
       "        2.92046626e+07, 2.91473193e+07, 2.90902183e+07, 2.90333586e+07,\n",
       "        2.89767392e+07, 2.89203590e+07, 2.88642171e+07, 2.88083125e+07,\n",
       "        2.87526441e+07, 2.86972110e+07, 2.86420122e+07, 2.85870466e+07,\n",
       "        2.85323133e+07, 2.84778113e+07, 2.84235396e+07, 2.83694973e+07,\n",
       "        2.83156833e+07, 2.82620968e+07, 2.82087367e+07, 2.81556022e+07,\n",
       "        2.81026921e+07, 2.80500057e+07, 2.79975420e+07, 2.79452999e+07,\n",
       "        2.78932786e+07, 2.78414772e+07, 2.77898946e+07, 2.77385301e+07,\n",
       "        2.76873826e+07, 2.76364513e+07, 2.75857352e+07, 2.75352334e+07,\n",
       "        2.74849451e+07, 2.74348693e+07, 2.73850051e+07, 2.73353516e+07,\n",
       "        2.72859079e+07, 2.72366733e+07, 2.71876466e+07, 2.71388272e+07,\n",
       "        2.70902140e+07, 2.70418064e+07, 2.69936032e+07, 2.69456038e+07,\n",
       "        2.68978072e+07, 2.68502127e+07, 2.68028192e+07, 2.67556261e+07,\n",
       "        2.67086323e+07, 2.66618372e+07, 2.66152398e+07, 2.65688394e+07,\n",
       "        2.65226350e+07, 2.64766259e+07, 2.64308112e+07, 2.63851902e+07,\n",
       "        2.63397619e+07, 2.62945256e+07, 2.62494805e+07, 2.62046258e+07,\n",
       "        2.61599606e+07, 2.61154841e+07, 2.60711956e+07, 2.60270943e+07,\n",
       "        2.59831793e+07, 2.59394500e+07, 2.58959054e+07, 2.58525449e+07,\n",
       "        2.58093676e+07, 2.57663727e+07, 2.57235596e+07, 2.56809274e+07,\n",
       "        2.56384753e+07, 2.55962027e+07, 2.55541087e+07, 2.55121925e+07,\n",
       "        2.54704536e+07, 2.54288910e+07, 2.53875040e+07, 2.53462920e+07,\n",
       "        2.53052541e+07, 2.52643896e+07, 2.52236979e+07, 2.51831781e+07,\n",
       "        2.51428295e+07, 2.51026515e+07, 2.50626432e+07, 2.50228040e+07,\n",
       "        2.49831332e+07, 2.49436300e+07, 2.49042938e+07, 2.48651238e+07,\n",
       "        2.48261193e+07, 2.47872797e+07, 2.47486042e+07, 2.47100921e+07,\n",
       "        2.46717428e+07, 2.46335556e+07, 2.45955297e+07, 2.45576646e+07,\n",
       "        2.45199594e+07, 2.44824136e+07, 2.44450265e+07, 2.44077973e+07,\n",
       "        2.43707255e+07, 2.43338104e+07, 2.42970512e+07, 2.42604475e+07,\n",
       "        2.42239983e+07, 2.41877033e+07, 2.41515616e+07, 2.41155726e+07,\n",
       "        2.40797358e+07, 2.40440503e+07, 2.40085157e+07, 2.39731313e+07,\n",
       "        2.39378964e+07, 2.39028104e+07, 2.38678726e+07, 2.38330825e+07,\n",
       "        2.37984395e+07, 2.37639428e+07, 2.37295919e+07, 2.36953862e+07,\n",
       "        2.36613251e+07, 2.36274078e+07, 2.35936340e+07, 2.35600028e+07,\n",
       "        2.35265138e+07, 2.34931663e+07, 2.34599597e+07, 2.34268934e+07,\n",
       "        2.33939669e+07, 2.33611796e+07, 2.33285308e+07, 2.32960199e+07,\n",
       "        2.32636465e+07, 2.32314099e+07, 2.31993095e+07, 2.31673447e+07,\n",
       "        2.31355151e+07, 2.31038199e+07, 2.30722587e+07, 2.30408309e+07,\n",
       "        2.30095359e+07, 2.29783731e+07, 2.29473421e+07, 2.29164421e+07,\n",
       "        2.28856728e+07, 2.28550335e+07, 2.28245237e+07, 2.27941428e+07,\n",
       "        2.27638903e+07, 2.27337656e+07, 2.27037682e+07, 2.26738977e+07,\n",
       "        2.26441533e+07, 2.26145347e+07, 2.25850412e+07, 2.25556723e+07,\n",
       "        2.25264276e+07, 2.24973064e+07, 2.24683084e+07, 2.24394328e+07,\n",
       "        2.24106793e+07, 2.23820473e+07, 2.23535363e+07, 2.23251458e+07,\n",
       "        2.22968753e+07, 2.22687243e+07, 2.22406922e+07, 2.22127785e+07,\n",
       "        2.21849829e+07, 2.21573047e+07, 2.21297434e+07, 2.21022987e+07,\n",
       "        2.20749699e+07, 2.20477566e+07, 2.20206583e+07, 2.19936746e+07,\n",
       "        2.19668049e+07, 2.19400487e+07, 2.19134056e+07, 2.18868751e+07,\n",
       "        2.18604567e+07, 2.18341499e+07, 2.18079543e+07, 2.17818694e+07,\n",
       "        2.17558948e+07, 2.17300299e+07, 2.17042744e+07, 2.16786276e+07,\n",
       "        2.16530893e+07, 2.16276589e+07, 2.16023359e+07, 2.15771200e+07,\n",
       "        2.15520106e+07, 2.15270074e+07, 2.15021098e+07, 2.14773174e+07,\n",
       "        2.14526298e+07, 2.14280465e+07, 2.14035671e+07, 2.13791912e+07,\n",
       "        2.13549182e+07, 2.13307479e+07, 2.13066797e+07, 2.12827132e+07,\n",
       "        2.12588480e+07, 2.12350836e+07, 2.12114197e+07, 2.11878558e+07,\n",
       "        2.11643914e+07, 2.11410263e+07, 2.11177598e+07, 2.10945917e+07,\n",
       "        2.10715215e+07, 2.10485488e+07, 2.10256731e+07, 2.10028942e+07,\n",
       "        2.09802115e+07, 2.09576246e+07, 2.09351333e+07, 2.09127369e+07,\n",
       "        2.08904352e+07, 2.08682278e+07, 2.08461142e+07, 2.08240940e+07,\n",
       "        2.08021670e+07, 2.07803325e+07, 2.07585904e+07, 2.07369401e+07,\n",
       "        2.07153813e+07, 2.06939137e+07, 2.06725367e+07, 2.06512501e+07,\n",
       "        2.06300535e+07, 2.06089464e+07, 2.05879285e+07, 2.05669995e+07,\n",
       "        2.05461589e+07, 2.05254063e+07, 2.05047415e+07, 2.04841640e+07,\n",
       "        2.04636735e+07, 2.04432695e+07, 2.04229518e+07, 2.04027199e+07,\n",
       "        2.03825736e+07, 2.03625123e+07, 2.03425359e+07, 2.03226439e+07,\n",
       "        2.03028359e+07, 2.02831117e+07, 2.02634708e+07, 2.02439129e+07,\n",
       "        2.02244376e+07, 2.02050447e+07, 2.01857337e+07, 2.01665043e+07,\n",
       "        2.01473562e+07, 2.01282890e+07, 2.01093024e+07, 2.00903960e+07,\n",
       "        2.00715695e+07, 2.00528226e+07, 2.00341549e+07, 2.00155661e+07,\n",
       "        1.99970559e+07, 1.99786239e+07, 1.99602697e+07, 1.99419932e+07,\n",
       "        1.99237938e+07, 1.99056714e+07, 1.98876256e+07, 1.98696560e+07,\n",
       "        1.98517624e+07, 1.98339443e+07, 1.98162016e+07, 1.97985339e+07,\n",
       "        1.97809408e+07, 1.97634221e+07, 1.97459774e+07, 1.97286064e+07,\n",
       "        1.97113089e+07, 1.96940844e+07, 1.96769327e+07, 1.96598536e+07,\n",
       "        1.96428466e+07, 1.96259114e+07, 1.96090478e+07, 1.95922555e+07,\n",
       "        1.95755342e+07, 1.95588835e+07, 1.95423032e+07, 1.95257930e+07,\n",
       "        1.95093525e+07, 1.94929815e+07, 1.94766797e+07, 1.94604468e+07,\n",
       "        1.94442825e+07, 1.94281865e+07, 1.94121585e+07, 1.93961982e+07,\n",
       "        1.93803054e+07, 1.93644798e+07, 1.93487210e+07, 1.93330289e+07,\n",
       "        1.93174030e+07, 1.93018432e+07, 1.92863491e+07, 1.92709206e+07,\n",
       "        1.92555572e+07, 1.92402587e+07, 1.92250249e+07, 1.92098555e+07,\n",
       "        1.91947502e+07, 1.91797087e+07, 1.91647308e+07, 1.91498162e+07,\n",
       "        1.91349646e+07, 1.91201757e+07, 1.91054494e+07, 1.90907853e+07,\n",
       "        1.90761832e+07, 1.90616427e+07, 1.90471638e+07, 1.90327460e+07,\n",
       "        1.90183891e+07, 1.90040930e+07, 1.89898572e+07, 1.89756816e+07,\n",
       "        1.89615659e+07, 1.89475098e+07, 1.89335132e+07, 1.89195757e+07,\n",
       "        1.89056971e+07, 1.88918771e+07, 1.88781156e+07, 1.88644122e+07,\n",
       "        1.88507667e+07, 1.88371789e+07, 1.88236485e+07, 1.88101753e+07,\n",
       "        1.87967591e+07, 1.87833995e+07, 1.87700964e+07, 1.87568495e+07,\n",
       "        1.87436586e+07, 1.87305234e+07, 1.87174437e+07, 1.87044193e+07,\n",
       "        1.86914500e+07, 1.86785354e+07, 1.86656755e+07, 1.86528699e+07,\n",
       "        1.86401184e+07, 1.86274208e+07, 1.86147768e+07, 1.86021863e+07,\n",
       "        1.85896490e+07, 1.85771647e+07, 1.85647331e+07, 1.85523540e+07,\n",
       "        1.85400273e+07, 1.85277527e+07, 1.85155299e+07, 1.85033588e+07,\n",
       "        1.84912392e+07, 1.84791707e+07, 1.84671533e+07, 1.84551866e+07,\n",
       "        1.84432705e+07, 1.84314048e+07, 1.84195892e+07, 1.84078236e+07,\n",
       "        1.83961076e+07, 1.83844412e+07, 1.83728241e+07, 1.83612561e+07,\n",
       "        1.83497369e+07, 1.83382665e+07, 1.83268445e+07, 1.83154708e+07,\n",
       "        1.83041451e+07, 1.82928673e+07, 1.82816372e+07, 1.82704545e+07,\n",
       "        1.82593191e+07, 1.82482307e+07, 1.82371893e+07, 1.82261944e+07,\n",
       "        1.82152460e+07, 1.82043439e+07, 1.81934879e+07, 1.81826778e+07,\n",
       "        1.81719133e+07, 1.81611943e+07, 1.81505206e+07, 1.81398921e+07,\n",
       "        1.81293084e+07, 1.81187695e+07, 1.81082751e+07, 1.80978250e+07,\n",
       "        1.80874192e+07, 1.80770573e+07, 1.80667391e+07, 1.80564646e+07,\n",
       "        1.80462335e+07, 1.80360457e+07, 1.80259009e+07, 1.80157989e+07,\n",
       "        1.80057397e+07, 1.79957230e+07, 1.79857486e+07, 1.79758163e+07,\n",
       "        1.79659261e+07, 1.79560776e+07, 1.79462707e+07, 1.79365053e+07,\n",
       "        1.79267812e+07, 1.79170981e+07, 1.79074560e+07, 1.78978546e+07,\n",
       "        1.78882938e+07, 1.78787734e+07, 1.78692932e+07, 1.78598531e+07,\n",
       "        1.78504529e+07, 1.78410924e+07, 1.78317715e+07, 1.78224900e+07,\n",
       "        1.78132476e+07, 1.78040444e+07, 1.77948800e+07, 1.77857544e+07,\n",
       "        1.77766673e+07, 1.77676186e+07, 1.77586082e+07, 1.77496359e+07,\n",
       "        1.77407014e+07, 1.77318048e+07, 1.77229457e+07, 1.77141240e+07,\n",
       "        1.77053397e+07, 1.76965924e+07, 1.76878822e+07, 1.76792087e+07,\n",
       "        1.76705719e+07, 1.76619716e+07, 1.76534076e+07, 1.76448798e+07,\n",
       "        1.76363881e+07, 1.76279322e+07, 1.76195121e+07, 1.76111276e+07,\n",
       "        1.76027785e+07, 1.75944647e+07, 1.75861860e+07, 1.75779423e+07,\n",
       "        1.75697334e+07, 1.75615593e+07, 1.75534197e+07, 1.75453144e+07,\n",
       "        1.75372435e+07, 1.75292066e+07, 1.75212037e+07, 1.75132346e+07,\n",
       "        1.75052992e+07, 1.74973973e+07, 1.74895289e+07, 1.74816936e+07,\n",
       "        1.74738915e+07, 1.74661224e+07, 1.74583861e+07, 1.74506825e+07,\n",
       "        1.74430114e+07, 1.74353728e+07, 1.74277664e+07, 1.74201922e+07,\n",
       "        1.74126500e+07, 1.74051397e+07, 1.73976611e+07, 1.73902141e+07,\n",
       "        1.73827986e+07, 1.73754144e+07, 1.73680614e+07, 1.73607395e+07,\n",
       "        1.73534485e+07, 1.73461884e+07, 1.73389589e+07, 1.73317600e+07,\n",
       "        1.73245915e+07, 1.73174533e+07, 1.73103453e+07, 1.73032673e+07,\n",
       "        1.72962192e+07, 1.72892009e+07, 1.72822123e+07, 1.72752532e+07,\n",
       "        1.72683235e+07, 1.72614231e+07, 1.72545518e+07, 1.72477096e+07,\n",
       "        1.72408963e+07, 1.72341118e+07, 1.72273560e+07, 1.72206287e+07,\n",
       "        1.72139298e+07, 1.72072593e+07, 1.72006169e+07, 1.71940027e+07,\n",
       "        1.71874163e+07, 1.71808578e+07, 1.71743271e+07, 1.71678239e+07,\n",
       "        1.71613482e+07, 1.71548998e+07, 1.71484788e+07, 1.71420848e+07,\n",
       "        1.71357179e+07, 1.71293779e+07, 1.71230646e+07, 1.71167781e+07,\n",
       "        1.71105181e+07, 1.71042846e+07, 1.70980774e+07, 1.70918965e+07,\n",
       "        1.70857416e+07, 1.70796128e+07, 1.70735099e+07, 1.70674328e+07,\n",
       "        1.70613813e+07, 1.70553555e+07, 1.70493550e+07, 1.70433800e+07,\n",
       "        1.70374302e+07, 1.70315055e+07, 1.70256059e+07, 1.70197312e+07,\n",
       "        1.70138814e+07, 1.70080562e+07, 1.70022557e+07, 1.69964797e+07,\n",
       "        1.69907281e+07, 1.69850008e+07, 1.69792977e+07, 1.69736187e+07,\n",
       "        1.69679638e+07, 1.69623327e+07, 1.69567254e+07, 1.69511418e+07,\n",
       "        1.69455818e+07, 1.69400453e+07, 1.69345322e+07, 1.69290424e+07,\n",
       "        1.69235757e+07, 1.69181322e+07, 1.69127117e+07, 1.69073141e+07,\n",
       "        1.69019394e+07, 1.68965873e+07, 1.68912578e+07, 1.68859509e+07,\n",
       "        1.68806664e+07, 1.68754042e+07, 1.68701643e+07, 1.68649465e+07,\n",
       "        1.68597508e+07, 1.68545770e+07, 1.68494251e+07, 1.68442949e+07,\n",
       "        1.68391864e+07, 1.68340996e+07, 1.68290342e+07, 1.68239902e+07,\n",
       "        1.68189676e+07, 1.68139661e+07, 1.68089858e+07, 1.68040266e+07,\n",
       "        1.67990883e+07, 1.67941709e+07, 1.67892742e+07, 1.67843983e+07,\n",
       "        1.67795429e+07, 1.67747081e+07, 1.67698937e+07, 1.67650997e+07,\n",
       "        1.67603259e+07, 1.67555723e+07, 1.67508387e+07, 1.67461252e+07,\n",
       "        1.67414316e+07, 1.67367579e+07, 1.67321038e+07, 1.67274695e+07,\n",
       "        1.67228547e+07, 1.67182595e+07, 1.67136836e+07, 1.67091271e+07,\n",
       "        1.67045899e+07, 1.67000718e+07, 1.66955728e+07, 1.66910929e+07,\n",
       "        1.66866318e+07, 1.66821896e+07, 1.66777662e+07, 1.66733615e+07,\n",
       "        1.66689754e+07, 1.66646078e+07, 1.66602587e+07, 1.66559280e+07,\n",
       "        1.66516156e+07, 1.66473214e+07, 1.66430453e+07, 1.66387873e+07,\n",
       "        1.66345473e+07, 1.66303253e+07, 1.66261210e+07, 1.66219346e+07,\n",
       "        1.66177658e+07, 1.66136146e+07, 1.66094810e+07, 1.66053649e+07,\n",
       "        1.66012661e+07, 1.65971847e+07, 1.65931205e+07, 1.65890735e+07,\n",
       "        1.65850436e+07, 1.65810308e+07, 1.65770348e+07, 1.65730558e+07,\n",
       "        1.65690936e+07, 1.65651481e+07, 1.65612194e+07, 1.65573072e+07,\n",
       "        1.65534115e+07, 1.65495323e+07, 1.65456695e+07, 1.65418230e+07,\n",
       "        1.65379928e+07, 1.65341788e+07, 1.65303809e+07, 1.65265990e+07]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_final, cost_history = gradient_descent(X, y, theta, learning_rate, n_iterations)\n",
    "\n",
    "gradient_descent(X, y, theta, learning_rate, n_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise :** \n",
    "1. Create a ``predictions`` variable that contains model(X, theta_final).\n",
    "2. Use matplotlib to display the x and y scatter plot.\n",
    "3. Use the plot method to display your predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "predictions = np.dot(X, theta_final)\n",
    "x = X[:, 0]\n",
    "\n",
    "plt.scatter(x, y, color='b')\n",
    "plt.plot(x, predictions, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like this. \n",
    "![](./assets/final_theta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not, change the learning rate and the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves\n",
    "To check if our Gradient Descent algorithm worked well, we observe the evolution of the cost function through iterations. We are supposed to obtain a curve that decreases with each iteration until it stagnates at a minimal level (close to zero). If the curve does not follow this pattern, then the learning_rate step may be too high, we should take a lower step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cost_history)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like this: \n",
    "![](./assets/learning_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this histogram, we can see that after 400 iterations, the model no longer learns and becomes constant. We can thus redefine the n_iterations to 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To evaluate the real performance of our model with a popular metric (for your boss, client, or colleagues) we can use the coefficient of determination, also known as $R^2$. It comes from the method of least squares. The closer the result is to 1, the better your model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_determination(y, pred):\n",
    "    u = ((y - pred)**2).sum()\n",
    "    v = ((y - y.mean())**2).sum()\n",
    "    return 1 - u/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9545029311262251"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_determination(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end\n",
    "Ok ok, you just built your own model of linear regression, do you realize that? \n",
    "This part was a bit theoretical, but it's essential to understand how it works.  \n",
    "![tired.gif](./assets/tired.gif)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de linearregression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
